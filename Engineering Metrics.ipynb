{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT CONFIGURATION\n",
    "This section should be populated with the following values:\n",
    "* Query - The JIRA JQL query to identify the relevant scope\n",
    "* Title - A description of the scope to include in the reports.\n",
    "* Days to Extrapolate - For forecasting completion & creation rates, how much historically data should be considered.  Generally, data should be considered in 7-day blocks to avoid any weekend bias.\n",
    "* Start Dates - The first date to include in reports.\n",
    "* End Dates - The last date to include in reports.  This date can be future dated.  All dates beyond the current date will be based on extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jira_reconnect = False # You will only be prompted to connect the first time.  If credentials are entered incorrectly, you can change this to true to reconnect.\n",
    "start_date = \"2024-08-01\"\n",
    "end_date = \"2024-10-29\"\n",
    "days_to_extrapolate = 0\n",
    "\n",
    "# query = \"resolutiondate >= -90d and project != IP\"\n",
    "# title = \"All Resolved Tickets\"\n",
    "\n",
    "# query = 'filter=10442'\n",
    "# title = 'Operator Success'\n",
    "\n",
    "# query = \"filter=10444\"\n",
    "# title = \"Payment Capture\"\n",
    "\n",
    "query = \"filter=10443\"\n",
    "title = \"Pilot Success\"\n",
    "\n",
    "# query = \"filter=10284\"\n",
    "# title = \"Pilotbase\"\n",
    "\n",
    "# query = \"project = FSP and updateddate >= -180d \"\n",
    "# title = \"All FSP\"\n",
    "\n",
    "# query = \"project in (WEB, FSP, PWD, LTP, IOS, CACCT) and (createddate >= -90d or resolutiondate >= -90d) \"\n",
    "# title = \"All Engineering\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP CONNECTIONS AND CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency install and import  -- do both pip and pip3 to avoid issues.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip -q install jira \n",
    "!pip3 -q install jira\n",
    "!pip -q install tqdm\n",
    "!pip3 -q install tqdm\n",
    "!pip -q install matplotlib\n",
    "!pip3 -q install matplotlib\n",
    "!pip -q install pandas\n",
    "!pip3 -q install pandas\n",
    "!pip -q install scikit-learn\n",
    "!pip3 -q install scikit-learn\n",
    "!pip -q install plotly\n",
    "!pip3 -q install plotly\n",
    "\n",
    "\n",
    "from jira import JIRA\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jira Connection\n",
    "from jira_fsp_extracts import jira_connect\n",
    "\n",
    "j = None if 'j' not in globals() else j\n",
    "j = jira_connect(prompt_for_reconnect = jira_reconnect, existing_connection = j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE VALUES\n",
    "from jira_references import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_burndown_chart() # GENERATE A BURNDOWN CHART WITH TOTAL SCOPE, RESOLVED SCOPE, REMAINING SCOPE\n",
    "def generate_burndown_chart(df, start_date=None, end_date=None, extrapolate_days=None, title=None):\n",
    "    \"\"\"\n",
    "    Generate a burndown chart from a DataFrame containing Jira issue data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns 'Created Date', 'Resolution Date', and 'Status'.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the burndown chart.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the start and end date for the burndown chart\n",
    "    if start_date is None:\n",
    "        start_date = df['Created Date'].min()\n",
    "    else: \n",
    "        start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "\n",
    "    if end_date is None:\n",
    "        end_date = df['Resolution Date'].max()\n",
    "    else: \n",
    "        end_date = pd.to_datetime(end_date).tz_localize(None)\n",
    "  \n",
    "    # Ensure both start_date and end_date are timezone-naive (or have the same timezone)\n",
    "    if start_date.tzinfo is not None:\n",
    "        start_date = start_date.tz_convert(None)\n",
    "    if end_date.tzinfo is not None:\n",
    "        end_date = end_date.tz_convert(None)\n",
    "        \n",
    "    now = pd.Timestamp('now').normalize()\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Burndown Chart'\n",
    "        \n",
    "    # Create a date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Initialize a DataFrame to hold the burndown data\n",
    "    burndown_data = pd.DataFrame(date_range, columns=['Date'])\n",
    "    burndown_data['Total Tasks'] = 0\n",
    "    burndown_data['Remaining Tasks'] = 0\n",
    "    burndown_data['Resolved Tasks'] = 0\n",
    "\n",
    "    # Initialize extrapolation values\n",
    "    daily_resolution_rate = None\n",
    "    daily_creation_rate = None\n",
    "\n",
    "    # Calculate the number of tasks remaining on each date\n",
    "    for i, row in burndown_data.iterrows():\n",
    "        date = row['Date']\n",
    "        \n",
    "        if date <= now:\n",
    "            # prior to today we have actuals\n",
    "            total_tasks = df[(df['Created Date'] <= date)].shape[0]\n",
    "            resolved_tasks = df[(df['Resolution Date'] <= date)].shape[0]\n",
    "            remaining_tasks = total_tasks - resolved_tasks\n",
    "            \n",
    "            burndown_data.at[i, 'Total Tasks'] = total_tasks\n",
    "            burndown_data.at[i, 'Remaining Tasks'] = remaining_tasks\n",
    "            burndown_data.at[i, 'Resolved Tasks'] = resolved_tasks\n",
    "            burndown_data.at[i, 'Total Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Resolved Tasks (F)'] = None\n",
    "        else:\n",
    "            # Extrapolate future data if extrapolate_days is provided\n",
    "            burndown_data.at[i, 'Total Tasks'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks'] = None\n",
    "            burndown_data.at[i, 'Resolved Tasks'] = None\n",
    "            burndown_data.at[i, 'Total Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Resolved Tasks (F)'] = None\n",
    "            \n",
    "            if extrapolate_days is not None:\n",
    "                # Calculate the trajectories the first time\n",
    "                if daily_resolution_rate is None or daily_creation_rate is None:           \n",
    "                    # Calculate the trajectory based on the last \"X\" days\n",
    "                    trajectory_period = min(extrapolate_days, len(date_range))\n",
    "                    recent_data = burndown_data[(burndown_data['Date'] <= now)].tail(trajectory_period)\n",
    "                    \n",
    "                    daily_resolution_rate = recent_data['Resolved Tasks'].diff().mean()\n",
    "                    daily_creation_rate = recent_data['Total Tasks'].diff().mean()\n",
    "                    print(f\"Extrapolated outcomes based on {trajectory_period} days\")\n",
    "                    print(f\"-- Daily Resolution Rate: {round(daily_resolution_rate,2)}\")\n",
    "                    print(f\"-- Daily Creation Rate: {round(daily_creation_rate,2)}\")\n",
    "                    \n",
    "                    burndown_data.at[i-1, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks']\n",
    "                    burndown_data.at[i-1, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Remaining Tasks']\n",
    "                    burndown_data.at[i-1, 'Resolved Tasks (F)'] = burndown_data.at[i-1, 'Resolved Tasks']\n",
    "\n",
    "                        \n",
    "                burndown_data.at[i, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] + daily_creation_rate\n",
    "                burndown_data.at[i, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Remaining Tasks (F)'] + daily_creation_rate - daily_resolution_rate\n",
    "                burndown_data.at[i, 'Resolved Tasks (F)'] = burndown_data.at[i-1, 'Resolved Tasks (F)'] + daily_resolution_rate\n",
    "            \n",
    "\n",
    "\n",
    "    # Plot the burndown chart\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Total Tasks'], color='red', linestyle='-', label='Total Tasks')\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Remaining Tasks'], color='orange', linestyle='-', label='Remaining Tasks')\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Resolved Tasks'], color='green', linestyle='-', label='Resolved Tasks')\n",
    "\n",
    "    if extrapolate_days is not None and (daily_resolution_rate is not None or daily_creation_rate is not None):\n",
    "        if not burndown_data['Total Tasks (F)'].isna().all:\n",
    "            plt.plot(burndown_data['Date'], burndown_data['Total Tasks (F)'], color='red', linestyle='dashed')\n",
    "        if not burndown_data['Remaining Tasks (F)'].isna().all:\n",
    "            plt.plot(burndown_data['Date'], burndown_data['Remaining Tasks (F)'], color='orange', linestyle='dashed')\n",
    "        if not burndown_data['Resolved Tasks (F)'].isna().all:\n",
    "            plt.plot(burndown_data['Date'], burndown_data['Resolved Tasks (F)'], color='green', linestyle='dashed')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tickets')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_stacked_bar_chart() # GENERATE A CHART WITH TOTAL SCOPE BROKEN DOWN BY STATUS\n",
    "def generate_stacked_bar_chart(df, start_date=None, end_date=None, extrapolate_days=None, title=None):\n",
    "    \"\"\"\n",
    "    Generate a stacked bar chart based on statuses over time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns 'Created Date', 'Resolution Date', and 'Status'.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the burndown chart.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the start and end date for the burndown chart\n",
    "    if start_date is None:\n",
    "        start_date = df['Created Date'].min()\n",
    "    else: \n",
    "        start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "\n",
    "    if end_date is None:\n",
    "        end_date = df['Resolution Date'].max()\n",
    "    else: \n",
    "        end_date = pd.to_datetime(end_date).tz_localize(None)\n",
    "  \n",
    "    # Ensure both start_date and end_date are timezone-naive (or have the same timezone)\n",
    "    if start_date.tzinfo is not None:\n",
    "        start_date = start_date.tz_convert(None)\n",
    "    if end_date.tzinfo is not None:\n",
    "        end_date = end_date.tz_convert(None)\n",
    "        \n",
    "    now = pd.Timestamp('now').normalize() + pd.Timedelta(days=1)\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Stacked Bar Status Chart'\n",
    "        \n",
    "    # Create a date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Initialize a DataFrame to hold the burndown data\n",
    "    burndown_data = pd.DataFrame(date_range, columns=['Date'])\n",
    "    burndown_data['Total Tasks'] = 0\n",
    "    burndown_data['PM Backlog Tasks'] = 0\n",
    "    burndown_data['Eng Backlog Tasks'] = 0\n",
    "    burndown_data['In Development Tasks'] = 0\n",
    "    burndown_data['In Validation Tasks'] = 0\n",
    "    burndown_data['Done Tasks'] = 0\n",
    "\n",
    "    # Initialize extrapolation values\n",
    "    daily_resolution_rate = None\n",
    "    daily_creation_rate = None\n",
    "\n",
    "    # Calculate the number of tasks remaining on each date\n",
    "    for i, row in burndown_data.iterrows():\n",
    "        date = row['Date']\n",
    "        \n",
    "        if date <= now:\n",
    "            # prior to today we have actuals\n",
    "            total_tasks = df[(df['Created Date'] <= date)].shape[0]\n",
    "            done_tasks = df[(df['Done Date'] <= date)].shape[0] \n",
    "            in_validation_tasks = df[(df['Validation Date'] <= date) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            in_development_tasks = df[(df['Development Date'] <= date) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            eng_backlog_tasks = df[(df['Eng Backlog Date'] <= date) & ((df['Development Date'].isna()) | (df['Development Date'] > date)) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            pm_backlog_tasks = df[(df['PM Backlog Date'] <= date) & ((df['Eng Backlog Date'].isna()) | (df['Eng Backlog Date'] > date)) & ((df['Development Date'].isna()) | (df['Development Date'] > date)) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            #pm_backlog_tasks = total_tasks - done_tasks - in_validation_tasks - in_development_tasks - eng_backlog_tasks\n",
    "            \n",
    "            burndown_data.at[i, 'Total Tasks'] = total_tasks\n",
    "            burndown_data.at[i, 'PM Backlog Tasks'] = pm_backlog_tasks\n",
    "            burndown_data.at[i, 'Eng Backlog Tasks'] = eng_backlog_tasks\n",
    "            burndown_data.at[i, 'In Development Tasks'] = in_development_tasks\n",
    "            burndown_data.at[i, 'In Validation Tasks'] = in_validation_tasks\n",
    "            burndown_data.at[i, 'Done Tasks'] = done_tasks\n",
    "            burndown_data.at[i, 'Total Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Done Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks (F)'] = None\n",
    "        else:\n",
    "            # Extrapolate future data if extrapolate_days is provided\n",
    "            burndown_data.at[i, 'Total Tasks'] = None\n",
    "            burndown_data.at[i, 'PM Backlog Tasks'] = None\n",
    "            burndown_data.at[i, 'Eng Backlog Tasks'] = None\n",
    "            burndown_data.at[i, 'In Development Tasks'] = None\n",
    "            burndown_data.at[i, 'In Validation Tasks'] = None\n",
    "            burndown_data.at[i, 'Done Tasks'] = None\n",
    "            burndown_data.at[i, 'Total Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Done Tasks (F)'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks (F)'] = None\n",
    "            \n",
    "            if extrapolate_days is not None:\n",
    "                # Calculate the trajectories the first time\n",
    "                if daily_resolution_rate is None or daily_creation_rate is None:           \n",
    "                    # Calculate the trajectory based on the last \"X\" days\n",
    "                    trajectory_period = min(extrapolate_days, len(date_range))\n",
    "                    recent_data = burndown_data[(burndown_data['Date'] <= now)].tail(trajectory_period)\n",
    "                    \n",
    "                    daily_resolution_rate = recent_data['Done Tasks'].diff().mean()\n",
    "                    daily_creation_rate = recent_data['Total Tasks'].diff().mean()\n",
    "                    print(f\"Extrapolated outcomes based on {trajectory_period} days\")\n",
    "                    print(f\"-- Daily Resolution Rate: {round(daily_resolution_rate,2)}\")\n",
    "                    print(f\"-- Daily Creation Rate: {round(daily_creation_rate,2)}\")\n",
    "                    \n",
    "                    burndown_data.at[i-1, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks']\n",
    "                    burndown_data.at[i-1, 'Done Tasks (F)'] = burndown_data.at[i-1, 'Done Tasks']\n",
    "                    burndown_data.at[i-1, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] - burndown_data.at[i-1, 'Done Tasks (F)']\n",
    "            \n",
    "                burndown_data.at[i, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] + daily_creation_rate\n",
    "                burndown_data.at[i, 'Done Tasks (F)'] = burndown_data.at[i-1, 'Done Tasks (F)'] + daily_resolution_rate\n",
    "                if burndown_data.at[i, 'Done Tasks (F)'] < 0:\n",
    "                    burndown_data.at[i, 'Done Tasks (F)'] = 0\n",
    "                burndown_data.at[i, 'Remaining Tasks (F)'] = burndown_data.at[i, 'Total Tasks (F)'] - burndown_data.at[i, 'Done Tasks (F)']\n",
    "\n",
    "    # For troubleshooting\n",
    "    burndown_data.to_csv(\"burndown_data.csv\")\n",
    "    \n",
    "    # Plot the burndown chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    ax.plot(burndown_data['Date'], burndown_data['Total Tasks'], color='black', linestyle='-', label='Total')\n",
    "\n",
    "    # Plot the stacked bars\n",
    "    ax.bar(burndown_data['Date'], burndown_data['PM Backlog Tasks'], label='PM Backlog', color='red')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['Eng Backlog Tasks'], bottom=burndown_data['PM Backlog Tasks'], label='Eng Backlog', color='orange')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['In Development Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks'], label='In Development', color='yellow')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['In Validation Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks']+burndown_data['In Development Tasks'], label='In Validation', color='green')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['Done Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks']+burndown_data['In Development Tasks']+burndown_data['In Validation Tasks'], label='Done', color='blue')\n",
    "\n",
    "    if extrapolate_days is not None and (daily_resolution_rate is not None or daily_creation_rate is not None):\n",
    "        forecasted_data = burndown_data[burndown_data['Done Tasks (F)'].notna()]\n",
    "        if not forecasted_data['Total Tasks (F)'].isna().all():\n",
    "            plt.plot(forecasted_data['Date'], forecasted_data['Total Tasks (F)'], color='black', linestyle='dashed')\n",
    "        if not forecasted_data['Done Tasks (F)'].isna().all():\n",
    "            ax.bar(forecasted_data['Date'], forecasted_data['Done Tasks (F)'], bottom=forecasted_data['Remaining Tasks (F)'], color='blue', alpha=0.4)\n",
    "        \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tickets')\n",
    "\n",
    "    y_limit = math.ceil(max(burndown_data['Total Tasks'].max(), burndown_data['Total Tasks (F)'].max()) + 2)\n",
    "    plt.ylim(0, y_limit)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bar_chart(df, column, title=None):\n",
    "    \"\"\"\n",
    "    Generate an interactive bar chart based on volume for an identified column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data.\n",
    "    column (string): Name of the column for the buckets used on the horizontal axis.\n",
    "    title (string): Title of the chart.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the interactive chart.\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        title = 'Bar Chart'\n",
    "\n",
    "    # Count the occurrences in the specified column\n",
    "    bin_counts = df[column].value_counts().sort_index()\n",
    "\n",
    "    # Create a DataFrame from the counts\n",
    "    data = bin_counts.reset_index()\n",
    "    data.columns = [column, 'Number']\n",
    "\n",
    "    # Create the interactive bar chart\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x=column,\n",
    "        y='Number',\n",
    "        title=title,\n",
    "        labels={column: column, 'Number': 'Number'},\n",
    "        hover_data={'Number': True}\n",
    "    )\n",
    "\n",
    "    # Update layout for better readability\n",
    "    fig.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        plot_bgcolor='white',\n",
    "        xaxis=dict(showgrid=False),\n",
    "        yaxis=dict(showgrid=True, gridcolor='lightgrey'),\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_whisker_chart() # GENERATE A WHISKER CHART BASED ON VOLUME FOR AN IDENTIFIED COLUMN\n",
    "def generate_whisker_chart(df, columns, ylabel=None, title=None):\n",
    "    \"\"\"\n",
    "    GENERATE A WHISKER CHART BASED ON SELECTED COLUMNS\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame \n",
    "    columns (string[]): Name of the columns for analysis.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the chart.\n",
    "    \"\"\"\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Bar Chart'\n",
    "        \n",
    "    # Create the box plot\n",
    "    # Define colors for the box plots\n",
    "    colors = {\n",
    "        'boxes': 'DarkOrange',\n",
    "        'whiskers': 'DarkOrange',\n",
    "        'medians': 'Blue',\n",
    "        'caps': 'Green'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    try:\n",
    "        boxplot = df[columns].boxplot(\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', color='DarkOrange'),\n",
    "            capprops=dict(color=colors['caps']),\n",
    "            whiskerprops=dict(color=colors['whiskers']),\n",
    "            flierprops=dict(markerfacecolor='red', marker='o', markersize=5, linestyle='none'),\n",
    "            medianprops=dict(color=colors['medians']),\n",
    "            vert=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"ValueError encountered: \", e)\n",
    "        print(\"Ensure that the DataFrame contains data to generate a plot for the specified columns.\")\n",
    "        # Optionally, you can return or raise an error or handle it differently\n",
    "        \n",
    "    plt.title(title)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_tickets_over_time_interactive(df, top_n=None, start_date=None, group_by='Assignee', measure_col=None, aggfunc='count', title=None, cumulative_agg=True, date_type=\"RESOLUTION\"):\n",
    "    date_type = date_type.upper().strip()\n",
    "    match date_type:\n",
    "        case \"RESOLUTION\" | \"RESOLVED\":\n",
    "            date_field = 'Resolution Week'\n",
    "            label = 'Resolved'\n",
    "        case \"CREATION\" | \"CREATED\":\n",
    "            date_field = 'Created Week'\n",
    "            label = 'Created'\n",
    "        case _:\n",
    "            print(f\"Unsupported date type '{date_type}'\")\n",
    "            return\n",
    "            \n",
    "    # Filter the DataFrame based on start_date if provided\n",
    "    if start_date is not None:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        df = df[df[date_field] >= start_date]\n",
    "\n",
    "    # Optionally filter to top N groups based on the group_by column\n",
    "    if top_n is not None:\n",
    "        # Depending on the measure, compute top N groups based on the specified aggregation\n",
    "        if measure_col is None or aggfunc == 'count':\n",
    "            top_groups = df[group_by].value_counts().nlargest(top_n).index\n",
    "        else:\n",
    "            # For sum or other measures, compute top N groups based on the measure_col\n",
    "            top_groups = df.groupby(group_by)[measure_col].agg(aggfunc).nlargest(top_n).index\n",
    "        df = df[df[group_by].isin(top_groups)]\n",
    "\n",
    "    # Group by 'Resolution Week' and the specified group_by column\n",
    "    if measure_col is None:\n",
    "        # Default to count\n",
    "        tickets_per_week = df.groupby([date_field, group_by]).size().reset_index(name='Value')\n",
    "        yaxis_title = f'Cumulative Number of Tickets {label}'\n",
    "    else:\n",
    "        # Use specified aggregation\n",
    "        tickets_per_week = df.groupby([date_field, group_by])[measure_col].agg(aggfunc).reset_index(name='Value')\n",
    "        yaxis_title = f'Cumulative {aggfunc} of {measure_col}'\n",
    "\n",
    "    # Pivot the data to have 'Resolution Week' as index and groups as columns\n",
    "    tickets_pivot = tickets_per_week.pivot(index=date_field, columns=group_by, values='Value').fillna(0)\n",
    "\n",
    "    # Sort the index to ensure the weeks are in chronological order\n",
    "    tickets_pivot = tickets_pivot.sort_index()\n",
    "\n",
    "    # Compute cumulative sums over time for each group\n",
    "    if cumulative_agg:\n",
    "        tickets_pivot = tickets_pivot.cumsum()\n",
    "\n",
    "    # Create a Plotly interactive line plot\n",
    "    if title is None:\n",
    "        title = f'Cumulative {aggfunc.capitalize()} Over Time per {group_by}'\n",
    "    fig = px.line(tickets_pivot, title=title, markers=True)\n",
    "\n",
    "    # Update layout for better readability\n",
    "    fig.update_layout(\n",
    "        xaxis_title=date_field,\n",
    "        yaxis_title=yaxis_title,\n",
    "        hovermode=\"x unified\",\n",
    "        legend_title=group_by,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    # If there is only one series, add trendline and calculate slope\n",
    "    if tickets_pivot.shape[1] == 1:\n",
    "        # Only one series\n",
    "        cumulative_values = tickets_pivot.iloc[:, 0]  # assuming only one column\n",
    "        series_name = cumulative_values.name  # Name of the series\n",
    "\n",
    "        # Compute week numbers since first week\n",
    "        first_week = tickets_pivot.index.min()\n",
    "        week_numbers = (tickets_pivot.index - first_week).days / 7  # fractional weeks\n",
    "\n",
    "        X = week_numbers.values.reshape(-1, 1)\n",
    "        y = cumulative_values.values\n",
    "\n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        slope = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "\n",
    "        # Print the slope after the chart\n",
    "        print(f\"Slope of the trendline: {slope:.2f} per week\")\n",
    "\n",
    "        # Create the trendline data\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        # Add trendline to the plot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=tickets_pivot.index,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            name='Trendline',\n",
    "            line=dict(dash='dash')\n",
    "        ))\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECT AND ANALYZE DATA FOR REPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE DATA\n",
    "from jira_fsp_extracts import fetch_jira_issues_to_dataframe\n",
    "\n",
    "final_query = '(' + query + ') and ((resolution is empty or resolution = Done) and status != \"Won\\'t Do\")'\n",
    "print(f\"JQL Query: {final_query}\")\n",
    "\n",
    "df = fetch_jira_issues_to_dataframe(jira_conn = j, jql_query = final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle time calculations\n",
    "# Defintions:\n",
    "# Lead Time: Created until Done\n",
    "    # Cycle Time: Development Start until Done\n",
    "        # Development Time: Development Start until Product Acceptance\n",
    "        # Dev Validation Time: Product Acceptance start until QA Start\n",
    "        # QA Validation Time: QA start until Done\n",
    "# + Release: Base Metric + Time from Done to Release (FUTURE)\n",
    "\n",
    "# only do calcs on done tickets\n",
    "df_ct_calcs = df[df['Done Date'].notna()].copy()\n",
    "if df_ct_calcs.shape[0] > 0:\n",
    "    df_ct_calcs[\"Lead Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['Created Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "    df_ct_calcs[\"Cycle Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['Development Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "    df_ct_calcs[\"CT Development Time (days)\"] = round((df_ct_calcs['Dev Validation Date'] - df_ct_calcs['Development Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "    df_ct_calcs[\"CT Dev Validation Time (days)\"] = round((df_ct_calcs['QA Validation Date'] - df_ct_calcs['Dev Validation Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "    df_ct_calcs[\"CT QA Validation Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['QA Validation Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "\n",
    "    # Calculate 'Release Time (days)' only when 'Release Date' is not None\n",
    "    df_ct_calcs[\"Release Time (days)\"] = df_ct_calcs.apply(\n",
    "        lambda row: round((row['Release Date'] - row['Done Date']).total_seconds() / (24 * 60 * 60), 2) if row['Release Date'] is not None else None,\n",
    "        axis=1)\n",
    "else:\n",
    "    df_ct_calcs[\"Lead Time (days)\"] = None\n",
    "    df_ct_calcs[\"Cycle Time (days)\"] = None\n",
    "    df_ct_calcs[\"CT Development Time (days)\"] = None\n",
    "    df_ct_calcs[\"CT Dev Validation Time (days)\"] = None\n",
    "    df_ct_calcs[\"CT QA Validation Time (days)\"] = None\n",
    "    df_ct_calcs[\"Release Time (days)\"] = None\n",
    "    \n",
    "# merge calced data into primary df\n",
    "# Drop the column if it exists\n",
    "df = df.drop(columns=[\"Lead Time (days)\",\"Cycle Time (days)\",\"CT Development Time (days)\",\"CT Dev Validation Time (days)\",\"CT QA Validation Time (days)\",\"Release Time (days)\"], errors='ignore')\n",
    "df = df.merge(df_ct_calcs[['Issue Key',\"Lead Time (days)\",\"Cycle Time (days)\",\"CT Development Time (days)\",\"CT Dev Validation Time (days)\",\"CT QA Validation Time (days)\",\"Release Time (days)\"]],on=\"Issue Key\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets for future reports\n",
    "df_all_issues = df\n",
    "df_all_issues_resolved_in_range = df[(df[\"Resolution Date\"] >= start_date) & (df[\"Resolution Date\"] < end_date)]\n",
    "df_all_issues_created_in_range = df[(df[\"Created Date\"] >= start_date) & (df[\"Created Date\"] < end_date)]\n",
    "\n",
    "df_standard_issues = df_all_issues[df_all_issues['Issue Type Category'] == \"STANDARD\"]\n",
    "df_standard_issues_resolved_in_range = df_all_issues_resolved_in_range[df_all_issues_resolved_in_range['Issue Type Category'] == \"STANDARD\"]\n",
    "df_standard_issues_created_in_range = df_all_issues_created_in_range[df_all_issues_created_in_range['Issue Type Category'] == \"STANDARD\"]\n",
    "\n",
    "df_sub_tasks_issues = df[df['Issue Type Category'] == \"SUBTASK\"]\n",
    "df_sub_tasks_issues_resolved_in_range = df_all_issues_resolved_in_range[df_all_issues_resolved_in_range['Issue Type Category'] == \"SUBTASK\"]\n",
    "df_sub_tasks_issues_created_in_range = df_all_issues_created_in_range[df_all_issues_created_in_range['Issue Type Category'] == \"SUBTASK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Defects / other subtasks\n",
    "defects = df_sub_tasks_issues_resolved_in_range[df_sub_tasks_issues_resolved_in_range['Issue Type'] == \"Defect\"].groupby(['Parent Story']).agg({'Issue Key': 'count'}).rename(columns={'Issue Key': 'Defect Count'})\n",
    "sub_tasks_analysis = df_sub_tasks_issues_resolved_in_range.groupby(['Parent Story']).agg({'Issue Key': 'count'}).rename(columns={'Issue Key': 'All Subtasks Count'})\n",
    "sub_tasks_analysis = sub_tasks_analysis.merge(defects, on=\"Parent Story\", how=\"left\").fillna(0)\n",
    "sub_tasks_analysis['Defect Percentage'] = (sub_tasks_analysis['Defect Count'] / sub_tasks_analysis['All Subtasks Count']) * 100\n",
    "\n",
    "# Bucket the percentages\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
    "sub_tasks_analysis['Defect Percentage Bin'] = pd.cut(sub_tasks_analysis['Defect Percentage'], bins=bins, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYCLE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_whisker_chart(df_standard_issues, \n",
    "                       columns=['Lead Time (days)', 'Cycle Time (days)', 'CT Development Time (days)', 'CT Dev Validation Time (days)', 'CT QA Validation Time (days)', 'Release Time (days)'], \n",
    "                       ylabel=\"Days\", title=\"Cycle Time Report - Stories\")\n",
    "generate_whisker_chart(df_sub_tasks_issues, \n",
    "                       columns=['Lead Time (days)', 'Cycle Time (days)', 'CT Development Time (days)', 'CT Dev Validation Time (days)', 'CT QA Validation Time (days)', 'Release Time (days)'], \n",
    "                       ylabel=\"Days\", title=\"Cycle Time Report - SubTasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUALITY METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Catch Rate - # of “Product Acceptance Changes” logged\n",
    "\n",
    "QA Catch Rate - # of Defects logged\n",
    "\n",
    "Escape rate - # of Bugs logged\n",
    "\n",
    "Customer Impact Rate - # of Bugs with Zendesk Ticket Count > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_bar_chart(sub_tasks_analysis,'Defect Percentage Bin',\"% of Defect Subtasks\")\n",
    "\n",
    "print(df_all_issues[\n",
    "    df_all_issues['Defect Category'].notna() & \n",
    "    (df_all_issues['Defect Category'].str.strip() != 'Other')].groupby(['Defect Category']).agg({'Issue Key': 'count'}))\n",
    "\n",
    "\n",
    "generate_bar_chart(df_all_issues_resolved_in_range[\n",
    "    df_all_issues_resolved_in_range['Defect Category'].notna() & \n",
    "    (df_all_issues_resolved_in_range['Defect Category'].str.strip() != 'Other')\n",
    "],'Defect Category',\"# of Tickets Resolved by Category\")\n",
    "\n",
    "generate_bar_chart(df_all_issues_created_in_range[\n",
    "    df_all_issues_created_in_range['Defect Category'].notna() & \n",
    "    (df_all_issues_created_in_range['Defect Category'].str.strip() != 'Other')\n",
    "],'Defect Category',\"# of Tickets Created by Category\")\n",
    "\n",
    "plot_tickets_over_time_interactive(df_standard_issues, top_n=5, start_date=start_date, group_by=\"Defect Category\", date_type=\"Resolved\")\n",
    "plot_tickets_over_time_interactive(df_standard_issues, top_n=5, start_date=start_date, group_by=\"Defect Category\", date_type=\"Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THROUGHPUT METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tickets_over_time_interactive(df_standard_issues_resolved_in_range, top_n=1, start_date=start_date, group_by=\"Status\", title=\"Team Velocity - # of Tickets (Standard Tickets)\")\n",
    "plot_tickets_over_time_interactive(df_sub_tasks_issues_resolved_in_range, top_n=1, start_date=start_date, group_by=\"Status\", title=\"Team Velocity - # of Tickets (Sub Tasks Tickets)\")\n",
    "plot_tickets_over_time_interactive(df_standard_issues_resolved_in_range, top_n=1, start_date=start_date, measure_col=\"Story Points\", aggfunc='sum', group_by=\"Status\", title=\"Team Velocity - Story Points\")\n",
    "plot_tickets_over_time_interactive(df_standard_issues_resolved_in_range, top_n=4, start_date=start_date, group_by=\"Assignee\", title=\"Resolved Tickets - By Individual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtasks Analysis\n",
    "plot_tickets_over_time_interactive(df_sub_tasks_issues_resolved_in_range, top_n=7, start_date=start_date, group_by=\"Assignee\", title=\"Resolved Subtasks - By Individual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "generate_bar_chart(df_all_issues[df_all_issues[\"Status Category\"].isin(['Development', 'Validation'])],'Status Category',\"Current WIP by Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release Velocity\n",
    "plot_tickets_over_time_interactive(df_standard_issues_resolved_in_range, top_n=10, start_date=start_date, measure_col=\"Release Version\", aggfunc='nunique', group_by=\"Status\", title=\"Number of Releases (Based on Fix Versions in JIRA)\", cumulative_agg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sub_tasks_issues[df_sub_tasks_issues['Resolution Week'] >= start_date].groupby(['Issue Type','Assignee']).agg({'Issue Key': 'count'})\n",
    "df_all_issues_resolved_in_range.to_csv(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
