{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT CONFIGURATION\n",
    "This section should be populated with the following values:\n",
    "* Query - The JIRA JQL query to identify the relevant scope\n",
    "* Title - A description of the scope to include in the reports.\n",
    "* Days to Extrapolate - For forecasting completion & creation rates, how much historically data should be considered.  Generally, data should be considered in 7-day blocks to avoid any weekend bias.\n",
    "* Start Dates - The first date to include in reports.\n",
    "* End Dates - The last date to include in reports.  This date can be future dated.  All dates beyond the current date will be based on extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'created >= -30d AND labels = Integration_Defects and issuekey != WEB-1892 '\n",
    "title = \"Log10 Integration Work\"\n",
    "days_to_extrapolate = 14\n",
    "start_date=\"2024-07-30\"\n",
    "end_date=\"2024-08-12\"\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# query = 'issuekey in portfolioChildIssuesOf(\"FSP-8899\")'\n",
    "# title = \"Dev Distractions Scope\"\n",
    "# days_to_extrapolate = 14\n",
    "# start_date=\"2024-07-01\"\n",
    "# end_date=\"2024-10-01\"\n",
    "\n",
    "# query = 'issuekey in portfolioChildIssuesOf(\"PWD-337\") or issuekey in portfolioChildIssuesOf(\"PWD-281\")'\n",
    "# title = \"Pilotbase Milestone 1 Scope\"\n",
    "# days_to_extrapolate = 14\n",
    "# start_date=\"2024-06-15\"\n",
    "# end_date=\"2024-09-01\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP CONNECTIONS AND CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency install and import \n",
    "!pip -q install jira\n",
    "!pip -q install tqdm\n",
    "!pip -q install matplotlib\n",
    "!pip -q install pandas\n",
    "\n",
    "from jira import JIRA\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Jira Connection\n",
    "needs_new_connection = False\n",
    "\n",
    "if 'j' in globals():\n",
    "    print(\"Do you want to reconnect to JIRA? ('Y' to reconnect)\")\n",
    "    reconnect = input()\n",
    "\n",
    "    if reconnect.strip().upper() in (\"1\",\"Y\",\"YES\"):\n",
    "        needs_new_connection = True\n",
    "else:\n",
    "    needs_new_connection = True\n",
    "        \n",
    "if needs_new_connection:\n",
    "    print(\"JIRA Username:\")\n",
    "    time.sleep(.25)\n",
    "    un = input()\n",
    "    print(\" \")\n",
    "    print(\"JIRA API Key (generated at https://id.atlassian.com/manage-profile/security/api-tokens):\")\n",
    "    time.sleep(.25)\n",
    "    pw = getpass.getpass()\n",
    "    \n",
    "    j = JIRA(\"https://flightschedulepro.atlassian.net/\", basic_auth=(un, pw))\n",
    "    pw = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# REFERENCE VALUES\n",
    "\n",
    "# Status Categories\n",
    "DONE_STATUSES = ['Done']\n",
    "IN_VALIDATION_STATUSES = ['QA Ready','In QA','Merged']\n",
    "IN_DEV_STATUSES = ['In Progress',  'Code Review', 'Development', 'In Development']\n",
    "ENG_BACKLOG_STATUSES = ['Selected for Development', 'Ready for Dev','Ready']\n",
    "PM_BACKLOG_STATUSES = ['Backlog', 'Ready for Refinement']\n",
    "\n",
    "# Issue Type Categories\n",
    "PLANNING_ISSUE_TYPES = ['Theme','Initiative']\n",
    "EPIC_ISSUE_TYPES = ['Epic']\n",
    "STANDARD_ISSUE_TYPES = ['Bug','DevOps Task','Release','Story','Support','Task','Test']\n",
    "SUB_TASK_ISSUE_TYPES = ['Defect','DevOps Sub-task','Integration Tests','Product Acceptance Change','Sub-task']\n",
    "\n",
    "# Fetch all fields and make a map of custom fields\n",
    "allfields = j.fields()\n",
    "fieldMap = {field['name']:field['id'] for field in allfields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def fetch_jira_issues_to_dataframe() # GET ALL TICKETS FOR A QUERY AND RETURN A DATAFRAME\n",
    "def fetch_jira_issues_to_dataframe(jira_conn, jql_query):\n",
    "    \"\"\"\n",
    "    Fetch issues from Jira based on a JQL query and return a DataFrame with specific fields.\n",
    "\n",
    "    Parameters:\n",
    "    jira_conn (JIRA): An authenticated JIRA connection object.\n",
    "    jql_query (str): The JQL query to fetch issues.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing issue key, status, resolution, created date, resolution date, issue type,\n",
    "                  parent story, parent epic, and parent initiative.\n",
    "    \"\"\"\n",
    "    issues = jira_conn.search_issues(jql_query, maxResults=False, expand='changelog', fields=\"key,summary,status,resolution,created,resolutiondate,issuetype,parent,customfield_10008,customfield_10009\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for issue in tqdm(issues, desc=\"Processing issues\"):\n",
    "        \n",
    "        issue_key = issue.key\n",
    "        issue_summary = issue.fields.summary\n",
    "        resolution = issue.fields.resolution.name if issue.fields.resolution else None\n",
    "       \n",
    "        # Issue Type values\n",
    "        issue_type = issue.fields.issuetype.name\n",
    "        issue_type_category = None\n",
    "        \n",
    "        if issue_type in PLANNING_ISSUE_TYPES:\n",
    "            issue_type_category = 'PLANNING'\n",
    "        elif issue_type in EPIC_ISSUE_TYPES:\n",
    "            issue_type_category = 'EPIC'\n",
    "        elif issue_type in STANDARD_ISSUE_TYPES:\n",
    "            issue_type_category = 'STANDARD'\n",
    "        elif issue_type in SUB_TASK_ISSUE_TYPES:\n",
    "            issue_type_category = 'SUBTASK'\n",
    "        else:\n",
    "            print(f\"Unable to make issue type {issue_type} to issue_type_category\")\n",
    "        \n",
    "        # Status values\n",
    "        status = issue.fields.status.name\n",
    "        status_category = None\n",
    "        if status in DONE_STATUSES:\n",
    "            status_category = 'Done'\n",
    "        elif status in IN_VALIDATION_STATUSES:\n",
    "            status_category = 'Validation'\n",
    "        elif status in IN_DEV_STATUSES:\n",
    "            status_category = 'Development'\n",
    "        elif status in ENG_BACKLOG_STATUSES:\n",
    "            status_category = 'Eng Backlog'\n",
    "        elif status in PM_BACKLOG_STATUSES:\n",
    "            status_category = 'PM Backlog'\n",
    "        else:\n",
    "            print(f\"Unable to map status {status} to status_category\")\n",
    "\n",
    "        \n",
    "        # Dates \n",
    "        created_date_raw = issue.fields.created\n",
    "        resolution_date_raw = issue.fields.resolutiondate if issue.fields.resolutiondate else None\n",
    "        \n",
    "        #print(f\"{issue_key} - {created_date_raw} - {resolution_date_raw}\")\n",
    "        created_date = pd.to_datetime(created_date_raw).tz_localize(None)\n",
    "        resolution_date = pd.to_datetime(resolution_date_raw).tz_localize(None) if resolution_date_raw else None\n",
    "\n",
    "        # Pull historical status change dates.\n",
    "        changelog = issue.changelog\n",
    "        # Loop through the changelog to find when the status changed to \"In Progress\"\n",
    "        pm_backlog_date = None\n",
    "        eng_backlog_date = None\n",
    "        dev_date = None\n",
    "        validation_date = None\n",
    "        done_date = None\n",
    "        \n",
    "        for history in changelog.histories:\n",
    "            changed_date = history.created\n",
    "            \n",
    "            for item in history.items:\n",
    "                changed_field = item.field\n",
    "                new_value = item.toString\n",
    "\n",
    "                if changed_field == \"status\":\n",
    "                    if new_value in DONE_STATUSES:\n",
    "                        if not done_date or changed_date > done_date:\n",
    "                            done_date = changed_date\n",
    "                    elif new_value in IN_VALIDATION_STATUSES:\n",
    "                        if not validation_date or changed_date > validation_date:\n",
    "                            validation_date = changed_date\n",
    "                    elif new_value in IN_DEV_STATUSES:\n",
    "                        if not dev_date or changed_date > dev_date:\n",
    "                            dev_date = changed_date\n",
    "                    elif new_value in ENG_BACKLOG_STATUSES:\n",
    "                        if not eng_backlog_date or changed_date > eng_backlog_date:\n",
    "                            eng_backlog_date = changed_date\n",
    "\n",
    "\n",
    "        # If the ticket is moved backwards, status dates can get weird.  Clean that up by removing dates from future stages\n",
    "        if status in IN_VALIDATION_STATUSES:\n",
    "            done_date = None\n",
    "        elif status in IN_DEV_STATUSES:\n",
    "            done_date = None\n",
    "            validation_date = None\n",
    "        elif status in ENG_BACKLOG_STATUSES:\n",
    "            done_date = None\n",
    "            validation_date = None\n",
    "            dev_date = None\n",
    "        elif status in PM_BACKLOG_STATUSES:\n",
    "            done_date = None\n",
    "            validation_date = None\n",
    "            dev_date = None\n",
    "            eng_backlog_date = None\n",
    "\n",
    "        # Normalize dates\n",
    "        done_date = pd.to_datetime(done_date).tz_localize(None) if done_date else None\n",
    "        validation_date = pd.to_datetime(validation_date).tz_localize(None) if validation_date else None\n",
    "        dev_date = pd.to_datetime(dev_date).tz_localize(None) if dev_date else None\n",
    "        eng_backlog_date = pd.to_datetime(eng_backlog_date).tz_localize(None) if eng_backlog_date else None\n",
    "        pm_backlog_date = created_date # default the PM backlog date to created\n",
    "\n",
    "        # Map hierarchical values\n",
    "        parent_story = None\n",
    "        parent_epic = None\n",
    "        parent_initiative = None\n",
    "        parent_theme = None\n",
    "\n",
    "        parent = issue.fields.parent.key if hasattr(issue.fields, 'parent') else None\n",
    "\n",
    "        if issue_type in PLANNING_ISSUE_TYPES:\n",
    "            issue_type_category = 'PLANNING'\n",
    "        elif issue_type in EPIC_ISSUE_TYPES:\n",
    "            issue_type_category = 'EPIC'\n",
    "        elif issue_type in STANDARD_ISSUE_TYPES:\n",
    "            issue_type_category = 'STANDARD'\n",
    "        elif issue_type in SUB_TASK_ISSUE_TYPES:\n",
    "            issue_type_category = 'SUBTASK'\n",
    "        else:\n",
    "            print(f\"Unable to make issue type {issue_type} to issue_type_category\")\n",
    "        \n",
    "        match issue_type_category:\n",
    "            case \"PLANNING\":\n",
    "                parent_theme = parent\n",
    "            case \"EPIC\":\n",
    "                parent_initiative = parent\n",
    "            case \"SUBTASK\":\n",
    "                # subtasks.  Parent = Story\n",
    "                parent_story = parent\n",
    "            case \"STANDARD\":\n",
    "                # standard issue type\n",
    "                parent_epic = parent\n",
    "            case _:\n",
    "                print(f\"Unable to map heirarchy values for issue {issue_key}\")\n",
    "                \n",
    " \n",
    "        data.append({\n",
    "            'Issue Key': issue_key,\n",
    "            'Summary': issue_summary,\n",
    "            'Status': status,\n",
    "            'Status Category': status_category,\n",
    "            'Resolution': resolution,\n",
    "            'Created Date': created_date,\n",
    "            'PM Backlog Date': pm_backlog_date,\n",
    "            'Eng Backlog Date': eng_backlog_date,\n",
    "            'Development Date': dev_date,\n",
    "            'Validation Date': validation_date,\n",
    "            'Done Date': done_date,\n",
    "            'Resolution Date': resolution_date,\n",
    "            'Issue Type': issue_type,\n",
    "            'Issue Type Category': issue_type_category,\n",
    "            'Parent Story': parent_story,\n",
    "            'Parent Story Name': None,\n",
    "            'Parent Epic': parent_epic,\n",
    "            'Parent Epic Name': None,\n",
    "            'Parent Initiative': parent_initiative,\n",
    "            'Parent Initiative Name': None,\n",
    "            'Parent Theme': parent_theme\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Second pass to populate the parent epic and parent initiative fields\n",
    "    for i, row in df.iterrows():\n",
    "        if row['Parent Initiative']:\n",
    "            parent_initiative_row = df[df['Issue Key'] == row['Parent Initiative']]\n",
    "            if not parent_initiative_row.empty:\n",
    "                df.at[i, 'Parent Initiative Name'] = parent_initiative_row['Summary'].values[0]\n",
    "                df.at[i, 'Parent Theme'] = parent_initiative_row['Parent Theme'].values[0]\n",
    "                \n",
    "    for i, row in df.iterrows():\n",
    "        if row['Parent Epic']:\n",
    "            parent_epic_row = df[df['Issue Key'] == row['Parent Epic']]\n",
    "            if not parent_epic_row.empty:\n",
    "                df.at[i, 'Parent Epic Name'] = parent_epic_row['Summary'].values[0]\n",
    "                df.at[i, 'Parent Initiative'] = parent_epic_row['Parent Initiative'].values[0]\n",
    "                df.at[i, 'Parent Initiative Name'] = parent_epic_row['Parent Initiative Name'].values[0]\n",
    "                df.at[i, 'Parent Theme'] = parent_epic_row['Parent Theme'].values[0]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row['Parent Story']:\n",
    "            parent_story_row = df[df['Issue Key'] == row['Parent Story']]\n",
    "            if not parent_story_row.empty:\n",
    "                df.at[i, 'Parent Story Name'] = parent_story_row['Summary'].values[0]\n",
    "                df.at[i, 'Parent Epic'] = parent_story_row['Parent Epic'].values[0]\n",
    "                df.at[i, 'Parent Epic Name'] = parent_story_row['Parent Epic Name'].values[0]\n",
    "                df.at[i, 'Parent Initiative'] = parent_story_row['Parent Initiative'].values[0]\n",
    "                df.at[i, 'Parent Initiative Name'] = parent_story_row['Parent Initiative Name'].values[0]\n",
    "                df.at[i, 'Parent Theme'] = parent_story_row['Parent Theme'].values[0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_burndown_chart() # GENERATE A BURNDOWN CHART WITH TOTAL SCOPE, RESOLVED SCOPE, REMAINING SCOPE\n",
    "def generate_burndown_chart(df, start_date=None, end_date=None, extrapolate_days=None, title=None):\n",
    "    \"\"\"\n",
    "    Generate a burndown chart from a DataFrame containing Jira issue data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns 'Created Date', 'Resolution Date', and 'Status'.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the burndown chart.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the start and end date for the burndown chart\n",
    "    if start_date is None:\n",
    "        start_date = df['Created Date'].min()\n",
    "    else: \n",
    "        start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "\n",
    "    if end_date is None:\n",
    "        end_date = df['Resolution Date'].max()\n",
    "    else: \n",
    "        end_date = pd.to_datetime(end_date).tz_localize(None)\n",
    "  \n",
    "    # Ensure both start_date and end_date are timezone-naive (or have the same timezone)\n",
    "    if start_date.tzinfo is not None:\n",
    "        start_date = start_date.tz_convert(None)\n",
    "    if end_date.tzinfo is not None:\n",
    "        end_date = end_date.tz_convert(None)\n",
    "        \n",
    "    now = pd.Timestamp('now').normalize()\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Burndown Chart'\n",
    "        \n",
    "    # Create a date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Initialize a DataFrame to hold the burndown data\n",
    "    burndown_data = pd.DataFrame(date_range, columns=['Date'])\n",
    "    burndown_data['Total Tasks'] = 0\n",
    "    burndown_data['Remaining Tasks'] = 0\n",
    "    burndown_data['Resolved Tasks'] = 0\n",
    "\n",
    "    # Initialize extrapolation values\n",
    "    daily_resolution_rate = None\n",
    "    daily_creation_rate = None\n",
    "\n",
    "    # Calculate the number of tasks remaining on each date\n",
    "    for i, row in burndown_data.iterrows():\n",
    "        date = row['Date']\n",
    "        \n",
    "        if date <= now:\n",
    "            # prior to today we have actuals\n",
    "            total_tasks = df[(df['Created Date'] <= date)].shape[0]\n",
    "            resolved_tasks = df[(df['Resolution Date'] <= date)].shape[0]\n",
    "            remaining_tasks = total_tasks - resolved_tasks\n",
    "            \n",
    "            burndown_data.at[i, 'Total Tasks'] = total_tasks\n",
    "            burndown_data.at[i, 'Remaining Tasks'] = remaining_tasks\n",
    "            burndown_data.at[i, 'Resolved Tasks'] = resolved_tasks\n",
    "        else:\n",
    "            # Extrapolate future data if extrapolate_days is provided\n",
    "            burndown_data.at[i, 'Total Tasks'] = None\n",
    "            burndown_data.at[i, 'Remaining Tasks'] = None\n",
    "            burndown_data.at[i, 'Resolved Tasks'] = None\n",
    "            \n",
    "            if extrapolate_days is not None:\n",
    "                # Calculate the trajectories the first time\n",
    "                if daily_resolution_rate is None or daily_creation_rate is None:           \n",
    "                    # Calculate the trajectory based on the last \"X\" days\n",
    "                    trajectory_period = min(extrapolate_days, len(date_range))\n",
    "                    recent_data = burndown_data[(burndown_data['Date'] <= now)].tail(trajectory_period)\n",
    "                    \n",
    "                    daily_resolution_rate = recent_data['Resolved Tasks'].diff().mean()\n",
    "                    daily_creation_rate = recent_data['Total Tasks'].diff().mean()\n",
    "                    print(f\"Extrapolated outcomes based on {trajectory_period} days\")\n",
    "                    print(f\"-- Daily Resolution Rate: {round(daily_resolution_rate,2)}\")\n",
    "                    print(f\"-- Daily Creation Rate: {round(daily_creation_rate,2)}\")\n",
    "                    \n",
    "                    burndown_data.at[i-1, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks']\n",
    "                    burndown_data.at[i-1, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Remaining Tasks']\n",
    "                    burndown_data.at[i-1, 'Resolved Tasks (F)'] = burndown_data.at[i-1, 'Resolved Tasks']\n",
    "\n",
    "                        \n",
    "                burndown_data.at[i, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] + daily_creation_rate\n",
    "                burndown_data.at[i, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Remaining Tasks (F)'] + daily_creation_rate - daily_resolution_rate\n",
    "                burndown_data.at[i, 'Resolved Tasks (F)'] = burndown_data.at[i-1, 'Resolved Tasks (F)'] + daily_resolution_rate\n",
    "\n",
    "\n",
    "    # Plot the burndown chart\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Total Tasks'], color='red', linestyle='-', label='Total Tasks')\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Remaining Tasks'], color='orange', linestyle='-', label='Remaining Tasks')\n",
    "    plt.plot(burndown_data['Date'], burndown_data['Resolved Tasks'], color='green', linestyle='-', label='Resolved Tasks')\n",
    "\n",
    "    if extrapolate_days is not None:\n",
    "        plt.plot(burndown_data['Date'], burndown_data['Total Tasks (F)'], color='red', linestyle='dashed')\n",
    "        plt.plot(burndown_data['Date'], burndown_data['Remaining Tasks (F)'], color='orange', linestyle='dashed')\n",
    "        plt.plot(burndown_data['Date'], burndown_data['Resolved Tasks (F)'], color='green', linestyle='dashed')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tickets')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def def generate_stacked_bar_chart() # GENERATE A CHART WITH TOTAL SCOPE BROKEN DOWN BY STATUS\n",
    "def generate_stacked_bar_chart(df, start_date=None, end_date=None, extrapolate_days=None, title=None):\n",
    "    \"\"\"\n",
    "    Generate a stacked bar chart based on statuses over time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns 'Created Date', 'Resolution Date', and 'Status'.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the burndown chart.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the start and end date for the burndown chart\n",
    "    if start_date is None:\n",
    "        start_date = df['Created Date'].min()\n",
    "    else: \n",
    "        start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "\n",
    "    if end_date is None:\n",
    "        end_date = df['Resolution Date'].max()\n",
    "    else: \n",
    "        end_date = pd.to_datetime(end_date).tz_localize(None)\n",
    "  \n",
    "    # Ensure both start_date and end_date are timezone-naive (or have the same timezone)\n",
    "    if start_date.tzinfo is not None:\n",
    "        start_date = start_date.tz_convert(None)\n",
    "    if end_date.tzinfo is not None:\n",
    "        end_date = end_date.tz_convert(None)\n",
    "        \n",
    "    now = pd.Timestamp('now').normalize() + pd.Timedelta(days=1)\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Stacked Bar Status Chart'\n",
    "        \n",
    "    # Create a date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Initialize a DataFrame to hold the burndown data\n",
    "    burndown_data = pd.DataFrame(date_range, columns=['Date'])\n",
    "    burndown_data['Total Tasks'] = 0\n",
    "    burndown_data['PM Backlog Tasks'] = 0\n",
    "    burndown_data['Eng Backlog Tasks'] = 0\n",
    "    burndown_data['In Development Tasks'] = 0\n",
    "    burndown_data['In Validation Tasks'] = 0\n",
    "    burndown_data['Done Tasks'] = 0\n",
    "\n",
    "    # Initialize extrapolation values\n",
    "    daily_resolution_rate = None\n",
    "    daily_creation_rate = None\n",
    "\n",
    "    # Calculate the number of tasks remaining on each date\n",
    "    for i, row in burndown_data.iterrows():\n",
    "        date = row['Date']\n",
    "        \n",
    "        if date <= now:\n",
    "            # prior to today we have actuals\n",
    "            total_tasks = df[(df['Created Date'] <= date)].shape[0]\n",
    "            done_tasks = df[(df['Done Date'] <= date)].shape[0] \n",
    "            in_validation_tasks = df[(df['Validation Date'] <= date) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            in_development_tasks = df[(df['Development Date'] <= date) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            eng_backlog_tasks = df[(df['Eng Backlog Date'] <= date) & ((df['Development Date'].isna()) | (df['Development Date'] > date)) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            pm_backlog_tasks = df[(df['PM Backlog Date'] <= date) & ((df['Eng Backlog Date'].isna()) | (df['Eng Backlog Date'] > date)) & ((df['Development Date'].isna()) | (df['Development Date'] > date)) & ((df['Validation Date'].isna()) | (df['Validation Date'] > date)) & ((df['Done Date'].isna()) | (df['Done Date'] > date))].shape[0] \n",
    "            #pm_backlog_tasks = total_tasks - done_tasks - in_validation_tasks - in_development_tasks - eng_backlog_tasks\n",
    "            \n",
    "            burndown_data.at[i, 'Total Tasks'] = total_tasks\n",
    "            burndown_data.at[i, 'PM Backlog Tasks'] = pm_backlog_tasks\n",
    "            burndown_data.at[i, 'Eng Backlog Tasks'] = eng_backlog_tasks\n",
    "            burndown_data.at[i, 'In Development Tasks'] = in_development_tasks\n",
    "            burndown_data.at[i, 'In Validation Tasks'] = in_validation_tasks\n",
    "            burndown_data.at[i, 'Done Tasks'] = done_tasks\n",
    "        else:\n",
    "            # Extrapolate future data if extrapolate_days is provided\n",
    "            burndown_data.at[i, 'Total Tasks'] = None\n",
    "            burndown_data.at[i, 'PM Backlog Tasks'] = None\n",
    "            burndown_data.at[i, 'Eng Backlog Tasks'] = None\n",
    "            burndown_data.at[i, 'In Development Tasks'] = None\n",
    "            burndown_data.at[i, 'In Validation Tasks'] = None\n",
    "            burndown_data.at[i, 'Done Tasks'] = None\n",
    "            \n",
    "            if extrapolate_days is not None:\n",
    "                # Calculate the trajectories the first time\n",
    "                if daily_resolution_rate is None or daily_creation_rate is None:           \n",
    "                    # Calculate the trajectory based on the last \"X\" days\n",
    "                    trajectory_period = min(extrapolate_days, len(date_range))\n",
    "                    recent_data = burndown_data[(burndown_data['Date'] <= now)].tail(trajectory_period)\n",
    "                    \n",
    "                    daily_resolution_rate = recent_data['Done Tasks'].diff().mean()\n",
    "                    daily_creation_rate = recent_data['Total Tasks'].diff().mean()\n",
    "                    print(f\"Extrapolated outcomes based on {trajectory_period} days\")\n",
    "                    print(f\"-- Daily Resolution Rate: {round(daily_resolution_rate,2)}\")\n",
    "                    print(f\"-- Daily Creation Rate: {round(daily_creation_rate,2)}\")\n",
    "                    \n",
    "                    burndown_data.at[i-1, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks']\n",
    "                    burndown_data.at[i-1, 'Done Tasks (F)'] = burndown_data.at[i-1, 'Done Tasks']\n",
    "                    burndown_data.at[i-1, 'Remaining Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] - burndown_data.at[i-1, 'Done Tasks (F)']\n",
    "            \n",
    "                burndown_data.at[i, 'Total Tasks (F)'] = burndown_data.at[i-1, 'Total Tasks (F)'] + daily_creation_rate\n",
    "                burndown_data.at[i, 'Done Tasks (F)'] = burndown_data.at[i-1, 'Done Tasks (F)'] + daily_resolution_rate\n",
    "                if burndown_data.at[i, 'Done Tasks (F)'] < 0:\n",
    "                    burndown_data.at[i, 'Done Tasks (F)'] = 0\n",
    "                burndown_data.at[i, 'Remaining Tasks (F)'] = burndown_data.at[i, 'Total Tasks (F)'] - burndown_data.at[i, 'Done Tasks (F)']\n",
    "\n",
    "\n",
    "    # Plot the burndown chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    ax.plot(burndown_data['Date'], burndown_data['Total Tasks'], color='black', linestyle='-', label='Total')\n",
    "\n",
    "    # Plot the stacked bars\n",
    "    ax.bar(burndown_data['Date'], burndown_data['PM Backlog Tasks'], label='PM Backlog', color='red')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['Eng Backlog Tasks'], bottom=burndown_data['PM Backlog Tasks'], label='Eng Backlog', color='orange')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['In Development Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks'], label='In Development', color='yellow')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['In Validation Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks']+burndown_data['In Development Tasks'], label='In Validation', color='green')\n",
    "    ax.bar(burndown_data['Date'], burndown_data['Done Tasks'], bottom=burndown_data['PM Backlog Tasks']+burndown_data['Eng Backlog Tasks']+burndown_data['In Development Tasks']+burndown_data['In Validation Tasks'], label='Done', color='blue')\n",
    "\n",
    "    if extrapolate_days is not None:\n",
    "        plt.plot(burndown_data['Date'], burndown_data['Total Tasks (F)'], color='black', linestyle='dashed')\n",
    "        ax.bar(burndown_data['Date'], burndown_data['Done Tasks (F)'], bottom=burndown_data['Remaining Tasks (F)'], color='blue', alpha=0.4)\n",
    "        #plt.plot(burndown_data['Date'], burndown_data['Done Tasks (F)'], color='blue', linestyle='dashed')\n",
    "        \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Tickets')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_bar_chart() # GENERATE A BAR CHART BASED ON VOLUME FOR AN IDENTIFIED COLUMN\n",
    "def generate_bar_chart(df, column, title=None):\n",
    "    \"\"\"\n",
    "    GENERATE A BAR CHART BASED ON VOLUME FOR AN IDENTIFIED COLUMN\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns 'Created Date', 'Resolution Date', and 'Status'.\n",
    "    column (string): Name of the column for the buckets used on the horizontal axis.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the chart.\n",
    "    \"\"\"\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Bar Chart'\n",
    "    \n",
    "    bin_counts = df[column].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Number')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_whisker_chart() # GENERATE A WHISKER CHART BASED ON VOLUME FOR AN IDENTIFIED COLUMN\n",
    "def generate_whisker_chart(df, columns, ylabel=None, title=None):\n",
    "    \"\"\"\n",
    "    GENERATE A WHISKER CHART BASED ON SELECTED COLUMNS\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame \n",
    "    columns (string[]): Name of the columns for analysis.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the chart.\n",
    "    \"\"\"\n",
    "    \n",
    "    if title is None:\n",
    "        title = 'Bar Chart'\n",
    "        \n",
    "    # Create the box plot\n",
    "    # Define colors for the box plots\n",
    "    colors = {\n",
    "        'boxes': 'DarkOrange',\n",
    "        'whiskers': 'DarkOrange',\n",
    "        'medians': 'Blue',\n",
    "        'caps': 'Green'\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    boxplot = df[columns].boxplot(\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='lightblue', color='DarkOrange'),\n",
    "        capprops=dict(color=colors['caps']),\n",
    "        whiskerprops=dict(color=colors['whiskers']),\n",
    "        flierprops=dict(markerfacecolor='red', marker='o', markersize=5, linestyle='none'),\n",
    "        medianprops=dict(color=colors['medians']),\n",
    "        vert=True\n",
    "    )\n",
    "    \n",
    "    plt.title(title)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECT AND ANALYZE DATA FOR REPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE DATA\n",
    "query = \"(\" + query + \") and (resolution is empty or resolution = Done)\"\n",
    "df = fetch_jira_issues_to_dataframe(j, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle time calculations\n",
    "# Defintions:\n",
    "# Lead Time: Created until Done\n",
    "    # Cycle Time: Development Start until Done\n",
    "        # Development Time: Development Start until QA Review\n",
    "        # Validation Time: QA Start Until Done\n",
    "# + Release: Base Metric + Time from Done to Release (FUTURE)\n",
    "\n",
    "# only do calcs on done tickets\n",
    "df_ct_calcs = df[df['Done Date'].notna()].copy()\n",
    "df_ct_calcs[\"Lead Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['Created Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "df_ct_calcs[\"Cycle Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['Development Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "df_ct_calcs[\"CT Development Time (days)\"] = round((df_ct_calcs['Validation Date'] - df_ct_calcs['Development Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "df_ct_calcs[\"CT Validation Time (days)\"] = round((df_ct_calcs['Done Date'] - df_ct_calcs['Validation Date']).dt.total_seconds() / (24 * 60 * 60),2)\n",
    "\n",
    "# merge calced data into primary df\n",
    "# Drop the column if it exists\n",
    "df = df.drop(columns=[\"Lead Time (days)\",\"Cycle Time (days)\",\"CT Development Time (days)\",\"CT Validation Time (days)\"], errors='ignore')\n",
    "df = df.merge(df_ct_calcs[['Issue Key',\"Lead Time (days)\",\"Cycle Time (days)\",\"CT Development Time (days)\",\"CT Validation Time (days)\"]],on=\"Issue Key\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets for future reports\n",
    "df_filtered_standard_issues = df[df['Issue Type Category'] == \"STANDARD\"]\n",
    "df_filtered_sub_tasks_issues = df[df['Issue Type Category'] == \"SUBTASK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Defects / other subtasks\n",
    "defects = df_filtered_sub_tasks_issues[df_filtered_sub_tasks_issues['Issue Type'] == \"Defect\"].groupby(['Parent Story']).agg({'Issue Key': 'count'}).rename(columns={'Issue Key': 'Defect Count'})\n",
    "sub_tasks_analysis = df_filtered_sub_tasks_issues.groupby(['Parent Story']).agg({'Issue Key': 'count'}).rename(columns={'Issue Key': 'All Subtasks Count'})\n",
    "sub_tasks_analysis = sub_tasks_analysis.merge(defects, on=\"Parent Story\", how=\"left\").fillna(0)\n",
    "sub_tasks_analysis['Defect Percentage'] = (sub_tasks_analysis['Defect Count'] / sub_tasks_analysis['All Subtasks Count']) * 100\n",
    "\n",
    "# Bucket the percentages\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
    "sub_tasks_analysis['Defect Percentage Bin'] = pd.cut(sub_tasks_analysis['Defect Percentage'], bins=bins, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BURNDOWN STACKED BAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stacked_bar_chart(df_filtered_standard_issues, start_date=start_date, end_date=end_date, extrapolate_days=days_to_extrapolate, title=title + ' - Story Burndown (Remaining scope should be 0 by launch date)')\n",
    "generate_stacked_bar_chart(df_filtered_sub_tasks_issues, start_date=start_date, end_date=end_date, extrapolate_days=days_to_extrapolate, title=title + ' - Sub-Task Burndown (Remaining scope should be stable in a flow state)')\n",
    "generate_stacked_bar_chart(df, start_date=start_date, end_date=end_date, extrapolate_days=days_to_extrapolate, title=title + ' - All Burndown (Remaining scope should be 0 by launch date)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFECTS PER STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bar_chart(sub_tasks_analysis,'Defect Percentage Bin',\"% of Defect Subtasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYCLE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_whisker_chart(df_filtered_standard_issues, \n",
    "                       columns=['Lead Time (days)', 'Cycle Time (days)', 'CT Development Time (days)', 'CT Validation Time (days)'], \n",
    "                       ylabel=\"Days\", title=\"Cycle Time Report - Stories\")\n",
    "generate_whisker_chart(df_filtered_sub_tasks_issues, \n",
    "                       columns=['Lead Time (days)', 'Cycle Time (days)', 'CT Development Time (days)', 'CT Validation Time (days)'], \n",
    "                       ylabel=\"Days\", title=\"Cycle Time Report - SubTasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
